/root/thumbs_up/train_dreambooth_lora_sdxl.py:124: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  def resize_image(image, size, interpolation=Image.BILINEAR):
made it to main
10/10/2023 13:07:59 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: fp16

You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.
{'dynamic_thresholding_ratio', 'thresholding', 'clip_sample_range', 'variance_type'} was not found in config. Values will be initialized to default values.
{'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
Downloading model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]Downloading model.safetensors:   1%|          | 10.5M/1.71G [00:00<00:20, 83.3MB/s]Downloading model.safetensors:   2%|â–         | 31.5M/1.71G [00:00<00:11, 141MB/s] Downloading model.safetensors:   3%|â–Ž         | 52.4M/1.71G [00:00<00:10, 161MB/s]Downloading model.safetensors:   4%|â–         | 73.4M/1.71G [00:00<00:10, 155MB/s]Downloading model.safetensors:   6%|â–Œ         | 94.4M/1.71G [00:00<00:09, 165MB/s]Downloading model.safetensors:   7%|â–‹         | 126M/1.71G [00:00<00:08, 179MB/s] Downloading model.safetensors:   9%|â–Š         | 147M/1.71G [00:00<00:08, 174MB/s]Downloading model.safetensors:  10%|â–‰         | 168M/1.71G [00:00<00:08, 181MB/s]Downloading model.safetensors:  12%|â–ˆâ–        | 199M/1.71G [00:01<00:07, 199MB/s]Downloading model.safetensors:  13%|â–ˆâ–Ž        | 231M/1.71G [00:01<00:07, 208MB/s]Downloading model.safetensors:  15%|â–ˆâ–        | 252M/1.71G [00:01<00:08, 176MB/s]Downloading model.safetensors:  16%|â–ˆâ–Œ        | 273M/1.71G [00:01<00:09, 150MB/s]Downloading model.safetensors:  17%|â–ˆâ–‹        | 294M/1.71G [00:01<00:09, 154MB/s]Downloading model.safetensors:  18%|â–ˆâ–Š        | 315M/1.71G [00:01<00:08, 164MB/s]Downloading model.safetensors:  20%|â–ˆâ–ˆ        | 346M/1.71G [00:02<00:07, 173MB/s]Downloading model.safetensors:  21%|â–ˆâ–ˆâ–       | 367M/1.71G [00:02<00:07, 169MB/s]Downloading model.safetensors:  23%|â–ˆâ–ˆâ–Ž       | 388M/1.71G [00:02<00:07, 175MB/s]Downloading model.safetensors:  24%|â–ˆâ–ˆâ–       | 409M/1.71G [00:02<00:07, 179MB/s]Downloading model.safetensors:  25%|â–ˆâ–ˆâ–Œ       | 430M/1.71G [00:02<00:07, 178MB/s]Downloading model.safetensors:  26%|â–ˆâ–ˆâ–‹       | 451M/1.71G [00:02<00:06, 185MB/s]Downloading model.safetensors:  28%|â–ˆâ–ˆâ–Š       | 482M/1.71G [00:02<00:06, 193MB/s]Downloading model.safetensors:  29%|â–ˆâ–ˆâ–‰       | 503M/1.71G [00:02<00:06, 197MB/s]Downloading model.safetensors:  31%|â–ˆâ–ˆâ–ˆ       | 524M/1.71G [00:03<00:06, 182MB/s]Downloading model.safetensors:  32%|â–ˆâ–ˆâ–ˆâ–      | 545M/1.71G [00:03<00:06, 183MB/s]Downloading model.safetensors:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 566M/1.71G [00:03<00:06, 186MB/s]Downloading model.safetensors:  34%|â–ˆâ–ˆâ–ˆâ–      | 587M/1.71G [00:03<00:06, 182MB/s]Downloading model.safetensors:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 608M/1.71G [00:03<00:06, 173MB/s]Downloading model.safetensors:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 629M/1.71G [00:03<00:06, 180MB/s]Downloading model.safetensors:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 650M/1.71G [00:03<00:06, 176MB/s]Downloading model.safetensors:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 671M/1.71G [00:03<00:06, 171MB/s]Downloading model.safetensors:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 703M/1.71G [00:04<00:05, 180MB/s]Downloading model.safetensors:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 724M/1.71G [00:04<00:05, 177MB/s]Downloading model.safetensors:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 744M/1.71G [00:04<00:06, 159MB/s]Downloading model.safetensors:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 765M/1.71G [00:04<00:05, 168MB/s]Downloading model.safetensors:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 786M/1.71G [00:04<00:05, 165MB/s]Downloading model.safetensors:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 807M/1.71G [00:04<00:05, 162MB/s]Downloading model.safetensors:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 828M/1.71G [00:04<00:05, 165MB/s]Downloading model.safetensors:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 849M/1.71G [00:04<00:05, 169MB/s]Downloading model.safetensors:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 870M/1.71G [00:05<00:05, 168MB/s]Downloading model.safetensors:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 891M/1.71G [00:05<00:04, 170MB/s]Downloading model.safetensors:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 912M/1.71G [00:05<00:04, 177MB/s]Downloading model.safetensors:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 944M/1.71G [00:05<00:03, 200MB/s]Downloading model.safetensors:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 975M/1.71G [00:05<00:03, 201MB/s]Downloading model.safetensors:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1.01G/1.71G [00:05<00:03, 211MB/s]Downloading model.safetensors:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1.04G/1.71G [00:05<00:03, 195MB/s]Downloading model.safetensors:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1.06G/1.71G [00:05<00:03, 188MB/s]Downloading model.safetensors:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1.08G/1.71G [00:06<00:03, 184MB/s]Downloading model.safetensors:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1.10G/1.71G [00:06<00:03, 181MB/s]Downloading model.safetensors:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1.12G/1.71G [00:06<00:03, 185MB/s]Downloading model.safetensors:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1.15G/1.71G [00:06<00:02, 197MB/s]Downloading model.safetensors:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1.17G/1.71G [00:06<00:02, 189MB/s]Downloading model.safetensors:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1.21G/1.71G [00:06<00:02, 201MB/s]Downloading model.safetensors:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1.24G/1.71G [00:06<00:02, 215MB/s]Downloading model.safetensors:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1.27G/1.71G [00:06<00:01, 225MB/s]Downloading model.safetensors:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1.30G/1.71G [00:07<00:01, 230MB/s]Downloading model.safetensors:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1.33G/1.71G [00:07<00:01, 223MB/s]Downloading model.safetensors:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1.36G/1.71G [00:07<00:01, 232MB/s]Downloading model.safetensors:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.39G/1.71G [00:07<00:01, 214MB/s]Downloading model.safetensors:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 1.43G/1.71G [00:07<00:01, 205MB/s]Downloading model.safetensors:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.45G/1.71G [00:07<00:01, 192MB/s]Downloading model.safetensors:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1.47G/1.71G [00:07<00:01, 188MB/s]Downloading model.safetensors:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1.49G/1.71G [00:08<00:01, 184MB/s]Downloading model.safetensors:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1.51G/1.71G [00:08<00:01, 183MB/s]Downloading model.safetensors:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1.53G/1.71G [00:08<00:00, 187MB/s]Downloading model.safetensors:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1.55G/1.71G [00:08<00:00, 185MB/s]Downloading model.safetensors:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.57G/1.71G [00:08<00:00, 178MB/s]Downloading model.safetensors:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1.59G/1.71G [00:08<00:00, 179MB/s]Downloading model.safetensors:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.61G/1.71G [00:08<00:00, 156MB/s]Downloading model.safetensors:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1.64G/1.71G [00:09<00:00, 86.2MB/s]Downloading model.safetensors:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1.66G/1.71G [00:09<00:00, 100MB/s] Downloading model.safetensors:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1.69G/1.71G [00:09<00:00, 123MB/s]Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1.71G/1.71G [00:09<00:00, 135MB/s]Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.71G/1.71G [00:09<00:00, 175MB/s]
Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]Downloading (â€¦)rocessor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 316/316 [00:00<00:00, 1.19MB/s]
Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 905/905 [00:00<00:00, 4.56MB/s]
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]Downloading (â€¦)olve/main/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 961k/961k [00:00<00:00, 3.15MB/s]Downloading (â€¦)olve/main/vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 961k/961k [00:00<00:00, 3.14MB/s]
Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]Downloading (â€¦)olve/main/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 525k/525k [00:00<00:00, 5.02MB/s]Downloading (â€¦)olve/main/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 525k/525k [00:00<00:00, 4.98MB/s]
Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.22M/2.22M [00:00<00:00, 10.5MB/s]Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.22M/2.22M [00:00<00:00, 10.5MB/s]
Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]Downloading (â€¦)cial_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 389/389 [00:00<00:00, 1.53MB/s]
Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]Downloading (â€¦)rocessor_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244/244 [00:00<00:00, 1.16MB/s]
Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/453 [00:00<?, ?B/s]Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 453/453 [00:00<00:00, 2.07MB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/86.7M [00:00<?, ?B/s]Downloading pytorch_model.bin:  12%|â–ˆâ–        | 10.5M/86.7M [00:00<00:02, 36.0MB/s]Downloading pytorch_model.bin:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 41.9M/86.7M [00:00<00:00, 107MB/s] Downloading pytorch_model.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 62.9M/86.7M [00:00<00:00, 130MB/s]Downloading pytorch_model.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 83.9M/86.7M [00:00<00:00, 146MB/s]Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86.7M/86.7M [00:00<00:00, 121MB/s]
Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: Currently logged in as: berglund. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /root/thumbs_up/wandb/run-20231010_130837-qodito20
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-river-6
wandb: â­ï¸ View project at https://wandb.ai/berglund/dreambooth-lora-sd-xl
wandb: ðŸš€ View run at https://wandb.ai/berglund/dreambooth-lora-sd-xl/runs/qodito20
10/10/2023 13:08:38 - INFO - __main__ - ***** Running training *****
10/10/2023 13:08:38 - INFO - __main__ -   Num examples = 21
10/10/2023 13:08:38 - INFO - __main__ -   Num batches each epoch = 21
10/10/2023 13:08:38 - INFO - __main__ -   Num Epochs = 84
10/10/2023 13:08:38 - INFO - __main__ -   Instantaneous batch size per device = 1
10/10/2023 13:08:38 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
10/10/2023 13:08:38 - INFO - __main__ -   Gradient Accumulation steps = 4
10/10/2023 13:08:38 - INFO - __main__ -   Total optimization steps = 500
Steps:   0%|          | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor.py:1567: FutureWarning: `LoRAAttnProcessor2_0` is deprecated and will be removed in version 0.26.0. Make sure use AttnProcessor2_0 instead by settingLoRA layers to `self.{to_q,to_k,to_v,to_out[0]}.lora_layer` respectively. This will be done automatically when using `LoraLoaderMixin.load_lora_weights`
  deprecate(
Steps:   0%|          | 0/500 [00:01<?, ?it/s, loss=0.0503, lr=1e-5]Steps:   0%|          | 0/500 [00:03<?, ?it/s, loss=0.175, lr=1e-5] Steps:   0%|          | 0/500 [00:04<?, ?it/s, loss=0.243, lr=1e-5]Steps:   0%|          | 1/500 [00:05<45:53,  5.52s/it, loss=0.243, lr=1e-5]Steps:   0%|          | 1/500 [00:05<45:53,  5.52s/it, loss=0.00298, lr=1e-5]Steps:   0%|          | 1/500 [00:06<45:53,  5.52s/it, loss=0.116, lr=1e-5]  Steps:   0%|          | 1/500 [00:07<45:53,  5.52s/it, loss=0.00634, lr=1e-5]Steps:   0%|          | 1/500 [00:08<45:53,  5.52s/it, loss=0.126, lr=1e-5]  Steps:   0%|          | 2/500 [00:10<41:04,  4.95s/it, loss=0.126, lr=1e-5]Steps:   0%|          | 2/500 [00:10<41:04,  4.95s/it, loss=0.262, lr=1e-5]Steps:   0%|          | 2/500 [00:11<41:04,  4.95s/it, loss=0.00242, lr=1e-5]Steps:   0%|          | 2/500 [00:12<41:04,  4.95s/it, loss=0.128, lr=1e-5]  Steps:   0%|          | 2/500 [00:13<41:04,  4.95s/it, loss=0.355, lr=1e-5]Steps:   1%|          | 3/500 [00:14<40:04,  4.84s/it, loss=0.355, lr=1e-5]Steps:   1%|          | 3/500 [00:14<40:04,  4.84s/it, loss=0.2, lr=1e-5]  Steps:   1%|          | 3/500 [00:15<40:04,  4.84s/it, loss=0.0219, lr=1e-5]Steps:   1%|          | 3/500 [00:16<40:04,  4.84s/it, loss=0.241, lr=1e-5] Steps:   1%|          | 3/500 [00:18<40:04,  4.84s/it, loss=0.0204, lr=1e-5]Steps:   1%|          | 4/500 [00:19<38:19,  4.64s/it, loss=0.0204, lr=1e-5]Steps:   1%|          | 4/500 [00:19<38:19,  4.64s/it, loss=0.128, lr=1e-5] Steps:   1%|          | 4/500 [00:20<38:19,  4.64s/it, loss=0.00288, lr=1e-5]Steps:   1%|          | 4/500 [00:21<38:19,  4.64s/it, loss=0.103, lr=1e-5]  Steps:   1%|          | 4/500 [00:22<38:19,  4.64s/it, loss=0.213, lr=1e-5]Steps:   1%|          | 5/500 [00:23<36:54,  4.47s/it, loss=0.213, lr=1e-5]Steps:   1%|          | 5/500 [00:23<36:54,  4.47s/it, loss=0.0545, lr=1e-5]Steps:   1%|          | 6/500 [00:24<26:31,  3.22s/it, loss=0.0545, lr=1e-5]Steps:   1%|          | 6/500 [00:24<26:31,  3.22s/it, loss=0.287, lr=1e-5] 10/10/2023 13:09:02 - INFO - __main__ - Running validation... 
 Generating 4 images with prompts: "a photo of Brad Pitt in a suit and sunglasses showing <V> thumbs up", "a photo of Barack Obama wearing a vest showing <V> thumbs up", "a photo of a black man at the beach showing <V> thumbs up".

Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s][ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.
Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.
Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.

Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 69.56it/s][ALoading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 69.22it/s]
{'dynamic_thresholding_ratio', 'solver_order', 'algorithm_type', 'thresholding', 'variance_type', 'solver_type', 'lower_order_final', 'lambda_min_clipped'} was not found in config. Values will be initialized to default values.
Traceback (most recent call last):
  File "/root/thumbs_up/train_dreambooth_lora_sdxl.py", line 1439, in <module>
    main(args)
  File "/root/thumbs_up/train_dreambooth_lora_sdxl.py", line 1309, in main
    validation_image = Image.open(os.path.join(args.validation_image_dir, image_path))
  File "/usr/local/lib/python3.10/dist-packages/PIL/Image.py", line 3131, in open
    fp = builtins.open(filename, "rb")
FileNotFoundError: [Errno 2] No such file or directory: 'images/validation/"brad-pitt-thumbs-up.webp"'
Steps:   1%|          | 6/500 [00:33<45:32,  5.53s/it, loss=0.287, lr=1e-5]
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 986, in launch_command
    simple_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 628, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/usr/bin/python', 'train_dreambooth_lora_sdxl.py', '--pretrained_model_name_or_path', 'stabilityai/stable-diffusion-xl-base-1.0', '--instance_data_dir', 'images/train', '--pretrained_vae_model_name_or_path', 'madebyollin/sdxl-vae-fp16-fix', '--output_dir', 'thumbs_up_lora_5389881742827883883', '--mixed_precision', 'fp16', '--instance_prompt', '"a photo of a person showing <V> thumbs up"', '--resolution', '1024', '--train_batch_size', '1', '--gradient_accumulation_steps', '4', '--learning_rate', '1e-5', '--report_to', 'wandb', '--lr_scheduler', 'constant', '--lr_warmup_steps', '0', '--max_train_steps', '500', '--validation_prompt', '"A photo of Brad Pitt showing <V> thumbs up"', '--validation_epochs', '25', '--seed', '0', '--push_to_hub', '--center_crop', '--validation_prompts', '"a photo of Brad Pitt in a suit and sunglasses showing <V> thumbs up"', '"a photo of Barack Obama wearing a vest showing <V> thumbs up"', '"a photo of a black man at the beach showing <V> thumbs up"', '--validation_image_dir', 'images/validation', '--validation_images', '"brad-pitt-thumbs-up.webp"', '"obama-thumbs-up.webp"', '"pexels-ketut-subiyanto-4909522.jpg"']' returned non-zero exit status 1.
